{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf1dfb0",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from thefuzz import process\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from patsy import dmatrix\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e6ab9",
   "metadata": {},
   "source": [
    "# Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('network_data.csv', index_col = 0)\n",
    "df = data.groupby(['source', 'target']).size().reset_index(name = 'weight')\n",
    "df = data.merge(df, on = ['source', 'target'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df = df,\n",
    "                            source = 'source',\n",
    "                            target = 'target',\n",
    "                            edge_attr = 'weight',\n",
    "                            create_using = nx.DiGraph()\n",
    "                            )\n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fff752",
   "metadata": {},
   "source": [
    "# Data for Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f45712",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "\n",
    "in_deg_cen = nx.in_degree_centrality(G)\n",
    "out_deg_cen = nx.out_degree_centrality(G)\n",
    "bet_cen = nx.betweenness_centrality(G)\n",
    "clo_cen = nx.closeness_centrality(G)\n",
    "eig_cen = nx.eigenvector_centrality(G)\n",
    "clu_coef = nx.clustering(G).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.DataFrame(columns = ['avg_ranking', 'in_degree_centrality', 'out_degree_centrality', \n",
    "                                     'betweenness_centrality', 'closeness_centrality', 'eigenvector_centrality',\n",
    "                                     'clustering_coefficient'], \n",
    "                          index = nodes)\n",
    "model_data['in_degree_centrality'] = in_deg_cen\n",
    "model_data['out_degree_centrality'] = out_deg_cen\n",
    "model_data['betweenness_centrality'] = bet_cen\n",
    "model_data['closeness_centrality'] = clo_cen\n",
    "model_data['eigenvector_centrality'] = eig_cen\n",
    "model_data['clustering_coefficient'] = clu_coef\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfade95",
   "metadata": {},
   "source": [
    "#### Load leagues standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "england = pd.read_csv('england_standings.csv', index_col = 0)\n",
    "italy = pd.read_csv('italy_standings.csv', index_col = 0)\n",
    "france = pd.read_csv('france_standings.csv', index_col = 0)\n",
    "netherlands = pd.read_csv('netherlands_standings.csv', index_col = 0)\n",
    "portugal = pd.read_csv('portugal_standings.csv', index_col = 0)\n",
    "russia = pd.read_csv('russia_standings.csv', index_col = 0)\n",
    "spain = pd.read_csv('spain_standings.csv', index_col = 0)\n",
    "germany = pd.read_csv('germany_standings.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aaf935",
   "metadata": {},
   "source": [
    "#### Compute the average ranking of each team over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876554fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "leagues = [england, italy, france, netherlands, portugal, russia, spain, germany]\n",
    "\n",
    "for league in leagues:\n",
    "    # Initialize an empty dictionary to store positions\n",
    "    positions = {team: [] for team in league.stack().unique()}\n",
    "\n",
    "    # Iterate over each column -> seasons and each row -> team positions\n",
    "    for season in league.columns:\n",
    "        for position, team in enumerate(league[season], start = 1):\n",
    "            if not pd.isna(team):\n",
    "                positions[team].append(position)\n",
    "\n",
    "    # Calculate the number of seasons each team is present in\n",
    "    num_seasons_present = {team: len(positions[team]) for team in positions}\n",
    "\n",
    "    # Determine the maximum number of seasons present in the league\n",
    "    max_seasons = max(num_seasons_present.values())\n",
    "\n",
    "    # Fill missing seasons with NaN values\n",
    "    for team, positions_list in positions.items():\n",
    "        num_missing_seasons = max_seasons - num_seasons_present[team]\n",
    "        if num_missing_seasons > 0:\n",
    "            positions[team] += [np.nan] * num_missing_seasons\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    positions_df = pd.DataFrame(positions)\n",
    "\n",
    "    # Calculate the average position for each team\n",
    "    average_positions = positions_df.mean(axis = 0, skipna = True)\n",
    "\n",
    "    # Create a DataFrame for average positions\n",
    "    average_positions_df = pd.DataFrame({'Team': positions_df.columns, 'Average Position': average_positions}).reset_index(drop = True)\n",
    "\n",
    "    # Sort the DataFrame by average position\n",
    "    average_positions_df = average_positions_df.sort_values(by = 'Average Position')\n",
    "    \n",
    "    results.append(average_positions_df)\n",
    "    \n",
    "standings = pd.DataFrame()\n",
    "\n",
    "for el in results:\n",
    "    standings = pd.concat([standings, el], ignore_index = True)\n",
    "    \n",
    "standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a459af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in model_data.index:\n",
    "    rank = standings.loc[standings['Team'] == team, 'Average Position']\n",
    "    model_data.loc[team, 'avg_ranking'] = rank.iloc[0]\n",
    "\n",
    "model_data['avg_ranking'] = pd.to_numeric(model_data['avg_ranking'], errors = 'coerce')\n",
    "model_data['in_degree_centrality'] = pd.to_numeric(model_data['in_degree_centrality'], errors = 'coerce')\n",
    "model_data['out_degree_centrality'] = pd.to_numeric(model_data['out_degree_centrality'], errors = 'coerce')\n",
    "model_data['betweenness_centrality'] = pd.to_numeric(model_data['betweenness_centrality'], errors = 'coerce')\n",
    "model_data['closeness_centrality'] = pd.to_numeric(model_data['closeness_centrality'], errors = 'coerce')\n",
    "model_data['eigenvector_centrality'] = pd.to_numeric(model_data['eigenvector_centrality'], errors = 'coerce')\n",
    "model_data['clustering_coefficient'] = pd.to_numeric(model_data['clustering_coefficient'], errors = 'coerce')\n",
    "\n",
    "# Final data \n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add45bc",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989f3fc",
   "metadata": {},
   "source": [
    "### OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[['in_degree_centrality', 'out_degree_centrality', \n",
    "                'betweenness_centrality', 'closeness_centrality', \n",
    "                'eigenvector_centrality', 'clustering_coefficient']]\n",
    "y = model_data['avg_ranking']\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "print(model.summary())\n",
    "print(' ')\n",
    "\n",
    "# Predict and calculate RMSE for train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(np.mean((y_train - y_train_pred) ** 2))\n",
    "test_rmse = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "print(' ')\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_train - y_train_pred\n",
    "plt.figure(figsize = (7, 5))\n",
    "sns.residplot(x = y_train_pred, y = residuals, lowess = True, line_kws = {'color': 'r', 'ls': '--'})\n",
    "plt.ylim((-10, 10))\n",
    "plt.xlim((0, 21))\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('OLS Residual Plot')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot for normality of residuals\n",
    "fig = plt.figure(figsize = (7, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "sm.qqplot(residuals, ax = ax)\n",
    "plt.ylim((-10, 7))\n",
    "plt.xlim((-10, 7))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "lim = np.array(ax.get_xlim())\n",
    "ax.plot(lim, lim, 'r--', lw=2)\n",
    "plt.title('OLS Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f517af",
   "metadata": {},
   "source": [
    "### OLS + Interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead54c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms\n",
    "interaction_model_data = dmatrix('in_degree_centrality + out_degree_centrality + \\\n",
    "                                  betweenness_centrality + closeness_centrality + \\\n",
    "                                  eigenvector_centrality + clustering_coefficient + \\\n",
    "                                  in_degree_centrality:out_degree_centrality + \\\n",
    "                                  in_degree_centrality:eigenvector_centrality + \\\n",
    "                                  out_degree_centrality:eigenvector_centrality', data = model_data, return_type = 'dataframe')\n",
    "y = model_data['avg_ranking']\n",
    "\n",
    "X = np.asarray(interaction_model_data)\n",
    "y = np.asarray(y)\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "print(model.summary())\n",
    "print(' ')\n",
    "\n",
    "# Predict and calculate RMSE for train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(np.mean((y_train - y_train_pred) ** 2))\n",
    "test_rmse = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "print(' ')\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_train - y_train_pred\n",
    "plt.figure(figsize = (7, 5))\n",
    "sns.residplot(x = y_train_pred, y = residuals, lowess = True, line_kws = {'color': 'r', 'ls': '--'})\n",
    "plt.ylim((-10, 10))\n",
    "plt.xlim((0, 21))\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('OLS + Interactions Residual Plot')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot for normality of residuals\n",
    "fig = plt.figure(figsize = (7, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "sm.qqplot(residuals, ax = ax)\n",
    "plt.ylim((-10, 7))\n",
    "plt.xlim((-10, 7))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "lim = np.array(ax.get_xlim())\n",
    "ax.plot(lim, lim, 'r--', lw=2)\n",
    "plt.title('OLS + Interactions Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4deb232",
   "metadata": {},
   "source": [
    "### Decision Tree, Random Forest, Gradient Boosting, Support Vector Regression and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b251dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[['in_degree_centrality', 'out_degree_centrality', \n",
    "                'betweenness_centrality', 'closeness_centrality', \n",
    "                'eigenvector_centrality', 'clustering_coefficient']]\n",
    "y = model_data['avg_ranking']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Models and their RMSEs\n",
    "models = {\n",
    "    'Regression Tree': DecisionTreeRegressor(max_depth = 5, random_state = 42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1, max_depth = 5, random_state = 42),\n",
    "    'Support Vector Regression': SVR(kernel = 'rbf', C = 1e3, gamma = 0.1),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes = (100, 100), max_iter = 1000, random_state = 42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_rmse = np.sqrt(np.mean((y_train - y_train_pred) ** 2))\n",
    "    test_rmse = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Train RMSE: {train_rmse}')\n",
    "    print(f'Test RMSE: {test_rmse}')\n",
    "    \n",
    "    residuals = y_train - y_train_pred\n",
    "    plt.figure(figsize = (7, 5))\n",
    "    sns.residplot(x = y_train_pred, y = residuals, lowess = True, line_kws = {'color': 'r', 'ls': '--'})\n",
    "    plt.ylim((-10, 10))\n",
    "    plt.xlim((0, 21))\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'{name} Residual Plot')\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize = (7, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sm.qqplot(residuals, ax = ax)\n",
    "    plt.ylim((-10, 7))\n",
    "    plt.xlim((-10, 7))\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    lim = np.array(ax.get_xlim())\n",
    "    ax.plot(lim, lim, 'r--', lw=2)\n",
    "    plt.title(f'{name} Q-Q Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fdd88",
   "metadata": {},
   "source": [
    "### Focus on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data[['in_degree_centrality', 'out_degree_centrality', \n",
    "                'betweenness_centrality', 'closeness_centrality', \n",
    "                'eigenvector_centrality', 'clustering_coefficient']]\n",
    "y = model_data['avg_ranking']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Random Forest Model\n",
    "model = RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(np.mean((y_train - y_train_pred) ** 2))\n",
    "test_rmse = np.sqrt(np.mean((y_test - y_test_pred) ** 2))\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize = (7, 5))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], align = \"center\", zorder = 3)\n",
    "plt.xticks(range(X.shape[1]), feature_names[indices], rotation = 90)\n",
    "plt.ylabel('Importance')\n",
    "plt.yticks([.10, .20, .30, .40])    \n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.grid(which = 'major', axis = 'y', zorder = 0, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X, feature_names = feature_names, plot_type = 'violin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
